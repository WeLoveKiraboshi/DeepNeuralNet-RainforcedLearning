{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVIS-DeepLearningLab_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2dvfidjkDSO"
      },
      "source": [
        "#Lab features:\n",
        "\n",
        "1. Keras for building model\n",
        "\n",
        "2. Tensorboard for monitoring training process\n",
        "\n",
        "3. Tensorflow eager execution\n",
        "\n",
        "#Note: \n",
        "To execute a cell, press `Shift+Enter`\n",
        "\n",
        "To change this Notebook, click on **File** at the top of this page then choose **Save a copy in Drive**. Then go to your Google Drive, and find the newly created file and open it with **Colab**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps8yMC39jpU4"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwF9hlRfjyzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4f37cb-0f05-4419-91f6-d0b2e423f3fd"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import datetime\n",
        "\n",
        "print(\"Tensorflow version: \", tf.__version__)\n",
        "tf.executing_eagerly()  # enable eager execution"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version:  2.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBrk29EKP-FK"
      },
      "source": [
        "# Helper function for plotting predictions\n",
        "\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"Pred {} {:2.0f}% (True {})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array, true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UjQYQ0VlpMd"
      },
      "source": [
        "#1. MNIST Dataset\n",
        "\n",
        "MNIST dataset is made of 70000 (60000 for training and 10000 for testing) grey scale images of handwritten digits. The digits are size-normalized and centered in a fixed-size image of size 28x28.\n",
        "\n",
        "Since Keras offers API for accessing MNIST, we can directly load this dataset to this Notebook while avoiding the harsh of parsing the raw dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCmx-A8gkQrN"
      },
      "source": [
        "# Load the dataset and split it to trainning set and test set\n",
        "mnist = keras.datasets.mnist\n",
        "# Split the dataset into trainning set and test set\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCIfo4g3mb6F"
      },
      "source": [
        "##1.1 Explore the data\n",
        "\n",
        "`train_images` is 3D tensor. Like an RGB image made of 3 channels (Red, Green, Blue) each represents an image, `train_images` has 60000 channels, each channel is a grey scale image of size 28x28."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg6xbCp7kq7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6574272c-e1b0-4850-89aa-7dea8ef2d323"
      },
      "source": [
        "print(\"shape of train_images: \", train_images.shape)\n",
        "print(\"type of train_images: \", type(train_images))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of train_images:  (60000, 28, 28)\n",
            "type of train_images:  <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G9Rus_mn844"
      },
      "source": [
        "`train_labels` stores the label of every image in `train_image` and is organized with the same order as `train_image`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tmUCVGRmtG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68db4b9c-be30-46f7-da19-4b9a54a6e8c7"
      },
      "source": [
        "print(\"shape of train_labels: \", train_labels.shape)\n",
        "print(\"type of train_images: \", type(train_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of train_labels:  (60000,)\n",
            "type of train_images:  <class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_aLCEsC3342",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2786c2-7a15-4119-c1b3-856dbc60c9e4"
      },
      "source": [
        "class_names = np.unique(train_labels)\n",
        "print(\"All labels in the dataset:\\n\", class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All labels in the dataset:\n",
            " [0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXmdDrDUoaND"
      },
      "source": [
        "Let's display some data samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5gt-CaBn537",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "outputId": "d2ad3986-489e-4fb2-f8c1-e343c47ce7b4"
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC98WfN2Iw_v"
      },
      "source": [
        "### Scale images to the range of (0, 1) before training and split train, validation set\n",
        "\n",
        "If this step is not done, the model can't learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYUD2xDxLeEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f68d3a1d-d7bd-4027-8788-ce9ca660f36b"
      },
      "source": [
        "train_images, test_images = train_images / 255.0, test_images / 255\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_images.shape)\n",
        "print(test_labels.shape)\n",
        "\n",
        "ratio = 0.01\n",
        "print('split train set to train:val={}:{}'.format(1-ratio, ratio))\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, val_images,train_labels, val_labels = train_test_split(train_images, train_labels, test_size=ratio)\n",
        "print(train_images.shape)\n",
        "print(val_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(val_labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n",
            "split train set to train:val=0.99:0.01\n",
            "(59400, 28, 28)\n",
            "(600, 28, 28)\n",
            "(59400,)\n",
            "(600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFV-xXys7hO_"
      },
      "source": [
        "#2. Fully Connected Network (FCN)\n",
        "\n",
        "An FCN shown below is a stack of fully (densely) connected layers. Each layer is an array of neurons, each of which carries out two operators. The first operator is the weighted sum of all input to a neuron, while the second is a non-linear function (e.g. sigmoid, relu).   \n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1qwdkn9y9L4S2Ue0NtUIRlplMFi6KAO1h'  width=\"450\" height=\"400\" />\n",
        "<figcaption>An example of FCN</figcaption></center>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGPU8h7UBPpt"
      },
      "source": [
        "##2.1 Building an FCN\n",
        "\n",
        "Here, a shallow FCN (with only one hidden layer) is built as an example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqRVjoChBPOI"
      },
      "source": [
        "def fcn(input_shape):\n",
        "  \"\"\"\n",
        "  Define a shallow FCN\n",
        "  Input:\n",
        "    input_shape [tuple]: shape of input image (height, width, channel)\n",
        "  Output:\n",
        "    model (a tensorflow model instance)\n",
        "  \"\"\"\n",
        "  \n",
        "  # Input layer\n",
        "  X_in = keras.layers.Input(shape=input_shape)\n",
        "  X = keras.layers.Flatten()(X_in)\n",
        "\n",
        "  # Hidden layer\n",
        "  X = keras.layers.Dense(512, activation='relu', name=\"fcn_hidden\")(X)\n",
        "\n",
        "  # Output layer\n",
        "  y = keras.layers.Dense(10, activation='softmax', name=\"fcn_output\")(X)\n",
        "\n",
        "  model = keras.Model(inputs=X_in, outputs=y, name=\"fcn\")\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoahw43yG6T0"
      },
      "source": [
        "Create a model from the function defined above and checkout model's parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGtQIlxx-z8K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9360ce9-18cb-473b-e97a-e5aa37999b7f"
      },
      "source": [
        "model_fcn = fcn((28, 28))\n",
        "model_fcn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"fcn\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "fcn_hidden (Dense)           (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "fcn_output (Dense)           (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP5K3aQIHavF"
      },
      "source": [
        "Compile the model by declaring the optimizer used for training and the loss function. The argument `metrics` are just for monitoring the training process, they don't have any effect on model training (i.e. adjusting model's weights to reduce value of loss function).   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZpiJRJiGUtv"
      },
      "source": [
        "#tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "model_fcn.compile(optimizer='Adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(), \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-66o2ijLxda"
      },
      "source": [
        "## 2.2 Training the model\n",
        "While training, the evolution of model's `loss` and `accuracy` are logged so that they can be displayed later using `Tensorboard`\n",
        "\n",
        "Note: the log file is stored locally in the machine (on the cloud) running this Notebook, therefore it will be erased when you close the web browser. To keep the log files, you can [mount your Google Drive into colab](https://colab.research.google.com/notebooks/io.ipynb) and set the variable `log_dir` to the path to the chosen Drive's folder. The path to your Drive should starts with `/content/drive/My Drive/`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBfY2IYOH1KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c817e6d2-a305-4826-b62a-61358e7c5ff1"
      },
      "source": [
        "%rm -rf logs/fit/fcn_m\n",
        "log_dir = \"logs/fit/\" + \"fcn_m\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping =  EarlyStopping(\n",
        "                            monitor='val_loss',\n",
        "                            min_delta=0.0,\n",
        "                            patience=5,\n",
        ")\n",
        "\n",
        "epoch = 2\n",
        "bs = 1\n",
        "# Fitting the dataset\n",
        "model_fcn.fit(x=train_images, \n",
        "          y=train_labels, \n",
        "          batch_size=bs,\n",
        "          epochs=epoch, \n",
        "          validation_data=(val_images, val_labels), \n",
        "          callbacks=[tensorboard_cb, early_stopping])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "59400/59400 [==============================] - 94s 2ms/step - loss: 0.3145 - accuracy: 0.9065 - val_loss: 0.1734 - val_accuracy: 0.9683\n",
            "Epoch 2/2\n",
            "59400/59400 [==============================] - 92s 2ms/step - loss: 0.1479 - accuracy: 0.9659 - val_loss: 0.2118 - val_accuracy: 0.9633\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f334fd1f8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJVY90CTMua6"
      },
      "source": [
        "Launch `Tensorboard` to check the evolution of loss and accuracy. This can take several seconds depend on your internet connection. \n",
        "\n",
        "In the GUI below, you can find several infomation about the model beside the `loss` and `accuracy` such as model's architecture in the `GRAPHS` window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yOf8d5ENFaD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "75485c30-f47f-4030-afa5-361d3afcd001"
      },
      "source": [
        "# you will need to change the date and time\n",
        "%tensorboard --logdir logs/fit/fcn_m"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 3755), started 0:06:12 ago. (Use '!kill 3755' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1628JoJpIzXa"
      },
      "source": [
        "##2.3 Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH0MA1ppI1VA"
      },
      "source": [
        "# Get model's prediction for test set\n",
        "predictions = model_fcn.predict(test_images)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzxv3gbiQLau"
      },
      "source": [
        "For a classification task, model outputs an array of probability of an image to belong to each class of object thanks to the output layer made of `softmax` activated neurons.\n",
        "\n",
        "Let's checkout the prediction for the first image in the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0AljdzHP72m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96e8df7-584b-4acd-f23d-e6d3aead66fd"
      },
      "source": [
        "print(\"Prediction for the first test image: \", predictions[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction for the first test image:  [1.2095875e-18 5.5982730e-22 2.8021715e-13 9.6133626e-13 3.4106982e-23\n",
            " 1.3157891e-16 2.2377844e-27 1.0000000e+00 6.3599019e-24 5.5595120e-17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThZ6Xr3DQ-N4"
      },
      "source": [
        "The class of object this image belong to is decided by the class which the model assigns the highest probability to"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OUtmJ5XQ9yF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "408af27c-ab04-4ca8-9b95-3bef40744a08"
      },
      "source": [
        "print(\"Predicted class of the first test image: \", np.argmax(predictions[0]))\n",
        "print(\"True label of the first test image: \", test_labels[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class of the first test image:  7\n",
            "True label of the first test image:  7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaobA4o6R4pY"
      },
      "source": [
        "To have a clearer visualization of model's predictions, let's display some test images along with their prediction and true labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tYJW8u7QG2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38114bbf-ede0-4ed2-8620-84a28e524f3d"
      },
      "source": [
        "# Plot the first X test images, their predicted labels, and the true labels.\n",
        "# Color correct predictions in blue and incorrect predictions in red.\n",
        "'''\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions[i], test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "'''\n",
        "score = model_fcn.evaluate(test_images, test_labels, verbose=0)\n",
        "print('*** test result for Fully Connected Networks ***')\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** test result for Fully Connected Networks ***\n",
            "Test loss: 0.2118808627128601\n",
            "Test accuracy: 0.9625999927520752\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIsUBvKDSmbv"
      },
      "source": [
        "#3. Convolution Neural Network (CNN)\n",
        "\n",
        "#3.1 Limitation of FCN:\n",
        "Depiste the FCN does great on MNIST testing dataset (98% of accuracy), FCN poses 3 major limitations. \n",
        "\n",
        "Firstly, FCNs do not scale well for images since the size of the first hidden layer is equal to the area of input image. To process a high resolution image, for example 1080p, of the size 1920×1080, the first hidden layer contains more than 1 million neurons.\n",
        "\n",
        "Secondly, the `Flatten` layer discards all the information brought by pixel position and the correlation between one pixel and its neighbors. This information is highly valuable for cognition task. The reason for this claim can be explained intuitively by fact that we get the context of an image by looking at a sufficiently big chunk of pixel, not by focusing on only one pixel.\n",
        "\n",
        "Finally, FCNs behave differently when objects appear at different positions. In other word, FCNs are not transition invariant. \"For example, if a picture of a cat appears in the top left of the image in one picture and the bottom right of another picture, the MLP will try to correct itself and assume that a cat will always appear in this section of the image.\" ([source](https://https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac))\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1Iau04cM3J_esZDTUVCrSBck7RmEHvhDe' width=\"450\" height=\"200\"/>\n",
        "<figcaption>A FCN-based cat detector adjusts its weights according to where the cat appears</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "These limitations call for a development of new learning architecture. The Convolution Layer - the core of Convolution Neural Network is the most successful answer to this call.\n",
        "\n",
        "#3.2 The Convolution Layer\n",
        "\n",
        "A convolution layer is made of a stack of **filters**. Each filter is stack of **kernels** with the number of kernels in the stack equal to the `depth` of input. A kernel is an 2D array of weights (i.e. matrix). A convolution layer takes in a tensor (a stack of matrix) of size `W_in x H_in x D_in` and produces an output tensor size `W_out x H_out x D_out`. Here, `W_out` and `H_out` are defined by kernels' size (this implied all kernels have the same size). `D_out` is the numer of filters, which means each filter contributes an **2D feature map** to the final output. A brief illustration of how convolution layer work can be found in [this video](https://youtu.be/jajksuQW4mc) made by Udacity.  \n",
        "\n",
        "Next the operation of convolution layer is illustrated.\n",
        "\n",
        "##3.2.1 Convolution with single channel input\n",
        "\n",
        "When inputs are gray scale images (such as MNIST dataset), the inputs' depth is 1, thus each filter is made of 1 kernel. In this particular case, the two terms **filters** and **kernels** are interchangable (which sometimes causes confusion). \n",
        "\n",
        "The 2D convolution illustrated in figure ([source](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1)) below is made of 3 steps:\n",
        "  * Slide a kernel across the input\n",
        "  * Element-wise multiplication\n",
        "  * Element-wise sum\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=10XjS0dOAw-mr9Y0-IUorcaPAzTwRmjDn'/>\n",
        "<figcaption>Convolution with single channel input.</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "This operation can be crossed check with Tensorflow's implementation of `Conv2D` layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSaQUvfv_XWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aa49603-4aec-4a1e-9b2b-ee269482829b"
      },
      "source": [
        "# Create a single-channel random tensor (i.e. a matrix) of size 5x5\n",
        "t1 = tf.random.uniform(shape=(5, 5), minval=0, maxval=255, dtype=tf.dtypes.float32, seed=7)\n",
        "print(\"t1 (before reshape): \\n\",t1)\n",
        "\n",
        "# reshape t1 so that it can be fed to a Conv2D layer\n",
        "# the 1st number of \"shape\" is the batch_size (the number of images fed to \n",
        "# the network at training time)\n",
        "# the 2nd number is input's width which is 5 in this case\n",
        "# the 3rd number is input's height (5 as well)\n",
        "# the last number is input's depth which is 1\n",
        "t1 = tf.reshape(t1, shape=[1, 5, 5, 1]) \n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t1 (before reshape): \n",
            " tf.Tensor(\n",
            "[[123.404945 129.08177   65.06166   31.809278 237.91757 ]\n",
            " [169.47537   58.452232 215.27794  196.1214   140.53638 ]\n",
            " [238.79706   27.853205 210.98756   42.35952   41.31403 ]\n",
            " [ 75.15212  135.05649   69.2159   224.77025   14.689526]\n",
            " [195.23306  206.26138   38.13442  150.9252    96.464874]], shape=(5, 5), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAyS0iluRu7S"
      },
      "source": [
        "# Create a Conv2D layer whose weights are randomly initialized\n",
        "# for testing the convolution operation, this layer has only 1 filter\n",
        "# the filter has 1 kernel (due to input's depth of 1) of size 3x3\n",
        "conv2d_single = keras.layers.Conv2D(filters=1, kernel_size=3, strides=(1, 1), padding='valid', name=\"conv2d_single\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uX1zQ51bAWN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c708544d-9eeb-4e16-b9c2-7fa7b156ebc4"
      },
      "source": [
        "# Get the ouput of applying conv2d_single on t1\n",
        "out_single = conv2d_single(t1)\n",
        "print(\"out_single: \\n\", out_single)\n",
        "\n",
        "# Since out_single is in the tensor form, its display is not clear,\n",
        "# let's convert it to numpy array and drop all the dimension equal to 1 (using squeeze())\n",
        "out_single_numpy = out_single.numpy().squeeze()\n",
        "print(\"\\nout_single_numpy (beautify): \\n\", out_single_numpy)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out_single: \n",
            " tf.Tensor(\n",
            "[[[[ 23.629393]\n",
            "   [207.20294 ]\n",
            "   [123.02942 ]]\n",
            "\n",
            "  [[200.93167 ]\n",
            "   [-29.830088]\n",
            "   [177.91472 ]]\n",
            "\n",
            "  [[230.36818 ]\n",
            "   [-67.68744 ]\n",
            "   [229.27101 ]]]], shape=(1, 3, 3, 1), dtype=float32)\n",
            "\n",
            "out_single_numpy (beautify): \n",
            " [[ 23.629393 207.20294  123.02942 ]\n",
            " [200.93167  -29.830088 177.91472 ]\n",
            " [230.36818  -67.68744  229.27101 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU_q1ZIDCyvL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88163a51-bfc8-4cb9-8925-9ab263348173"
      },
      "source": [
        "# Get the kernel\n",
        "weights = conv2d_single.get_weights()\n",
        "print(\"weights is a \", type(weights), \" of \", type(weights[0]))\n",
        "print(\"Number of filters: \", len(weights))\n",
        "\n",
        "# Even though weights contains 2 elements, the last elements is empty,\n",
        "# Hence the true number of fitlers is 1\n",
        "if not weights[1]:\n",
        "  print(\"\\nThe 2nd element of weights is empty.\")\n",
        "  print(\"The true number of filters: \", len(weights)-1)\n",
        "\n",
        "# Get the kernel of the 1st (and only) filter\n",
        "kernel = weights[0].squeeze()\n",
        "print(\"\\nKernel: \\n\", kernel)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights is a  <class 'list'>  of  <class 'numpy.ndarray'>\n",
            "Number of filters:  2\n",
            "\n",
            "The 2nd element of weights is empty.\n",
            "The true number of filters:  1\n",
            "\n",
            "Kernel: \n",
            " [[ 0.5294268  -0.5731855   0.02316707]\n",
            " [ 0.33785427  0.26588905  0.04181373]\n",
            " [-0.22350451  0.46731424 -0.05056846]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKg_tpT8DXQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "155c039d-7bf0-477a-8f85-b3aa99e48895"
      },
      "source": [
        "# To validate the theory explained at the beginning of this sub section,\n",
        "# let's try calculating the first number in the conv2d_single's output\n",
        "\n",
        "# Since the Conv2D layer is defined with \"valid\" padding, the first number in its\n",
        "# output is produce by convolving the top-left corner 3x3 block of the input\n",
        "# with the kernel\n",
        "\n",
        "# Proceed the 2nd step of convolution - element-wise multiplication\n",
        "# Note that, t1 has been reshaped into a 4D tensor so that it can be fed into\n",
        "# the Conv2D layer. \n",
        "mul_ = t1[0, 0:3, 0:3, 0] * kernel\n",
        "print(\"Element-wise multiplaicaiton:\\n\", mul_)\n",
        "\n",
        "# 3rd step: element-wise sum\n",
        "sum_ = tf.math.reduce_sum(mul_)\n",
        "\n",
        "# Check the output\n",
        "# This output should be close the first element of out_single_numpy \n",
        "assert(np.abs(sum_ - out_single_numpy[0, 0]) < 1e-3)\n",
        "print(\"\\nManual convolution result: \", sum_.numpy())"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Element-wise multiplaicaiton:\n",
            " tf.Tensor(\n",
            "[[ 65.333885  -73.9878      1.5072883]\n",
            " [ 57.257977   15.541808    9.001574 ]\n",
            " [-53.37222    13.016199  -10.669316 ]], shape=(3, 3), dtype=float32)\n",
            "\n",
            "Manual convolution result:  23.629398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBJo2Sm4FBjL"
      },
      "source": [
        "###3.2.2 Convolution with multi channels input\n",
        "\n",
        "In this case, each **filter** is a stack of **kernels**. A filter's depth is equal to the depth of input images. \n",
        "\n",
        "For a filter to produce a **2D feature map**, each of its kernels is applied to the inputs' channel at the same depth to produce an *intermediate* feature map. These intermediate feature maps are then sum element-wise to result in the *final* feature map. This process is illustrated below ([source](https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215))\n",
        "\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1WinRqO7DV1qRwYUJkMV8n4wowCw0yURH' height=\"150\"/>\n",
        "</figure>\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?id=1GfDPMf7IHVonG-em6VJUT_Pj4REbcIp4' height=\"150\"/>\n",
        "<figcaption>Convolution with multi channels input.</figcaption></center>\n",
        "</figure>\n",
        "\n",
        "To check this definition, let's again calculate the output of Tensorflow Conv2D layer and compare it to a manual construction Conv2D layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuiq4hz9EkV_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "580a2718-1cd8-4c4d-f513-5108380ba37d"
      },
      "source": [
        "# Create a random tensor of size 5x5x3\n",
        "t2 = tf.random.uniform(shape=(5, 5, 3), minval=0, maxval=255, dtype=tf.dtypes.float32, seed=7)\n",
        "\n",
        "# checkout the first channel\n",
        "print(\"First channel:\\n\", t2[:, :, 0])\n",
        "\n",
        "# reshape t2 into a 4D tensor so that it can be fed into Conv2D layer\n",
        "t2 = tf.reshape(t2, shape=(1, 5, 5, 3))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First channel:\n",
            " tf.Tensor(\n",
            "[[214.96742   86.603065  75.75371   72.984116  62.40697 ]\n",
            " [201.86253  146.16362  191.15242   11.084066  98.09435 ]\n",
            " [210.99307  157.7478   171.66254  107.12829   11.667259]\n",
            " [144.01303  200.86243  198.87437   36.742752 130.67278 ]\n",
            " [ 34.1353    65.48064  227.48932  124.03389  106.71174 ]], shape=(5, 5), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIwE-EP0FLFh"
      },
      "source": [
        "# create a new Conv2D layer\n",
        "conv2d_multi = tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=(1, 1), padding='valid', name=\"conv2d_multi\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLyKO0sGFLDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2e164d-99cf-4c34-b780-fc477e7ad345"
      },
      "source": [
        "# Feed t2 into conv2d_multi and checkout the output\n",
        "out_multi = conv2d_multi(t2)\n",
        "\n",
        "out_multi_numpy = out_multi.numpy().squeeze()\n",
        "print(\"\\nout_multi_numpy (beautify): \\n\", out_multi_numpy)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "out_multi_numpy (beautify): \n",
            " [[343.69336 351.8245  252.60553]\n",
            " [440.44583 207.86429 357.6176 ]\n",
            " [244.93178 272.03976 256.08008]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUkUCOB7FLAk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa04b994-cec5-451a-c1fe-e978f3566482"
      },
      "source": [
        "# Get filter\n",
        "weights_multi = conv2d_multi.get_weights()\n",
        "filter_ = weights_multi[0]\n",
        "print(\"Shape of filter_: \", filter_.shape)\n",
        "\n",
        "# Display kernels\n",
        "for i in range(filter_.shape[2]):\n",
        "  print(\"\\nkernel %d: \\n\" % i, filter_[:, :, 0].squeeze())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of filter_:  (3, 3, 3, 1)\n",
            "\n",
            "kernel 0: \n",
            " [[ 0.08372426 -0.22537732  0.31639326]\n",
            " [ 0.18549782 -0.20254579 -0.34321335]\n",
            " [-0.02813849  0.0880959   0.22336447]]\n",
            "\n",
            "kernel 1: \n",
            " [[ 0.08372426 -0.22537732  0.31639326]\n",
            " [ 0.18549782 -0.20254579 -0.34321335]\n",
            " [-0.02813849  0.0880959   0.22336447]]\n",
            "\n",
            "kernel 2: \n",
            " [[ 0.08372426 -0.22537732  0.31639326]\n",
            " [ 0.18549782 -0.20254579 -0.34321335]\n",
            " [-0.02813849  0.0880959   0.22336447]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGmVf-eCFK7s"
      },
      "source": [
        "# Mannually calculate the Convolution between t2 and the filter made of 3 kernels above\n",
        "# this calculation is just to check the first element of out_multi_numpy\n",
        "# which is the result of convolution with the top-left 3x3 block of t2\n",
        "\n",
        "sum_0 = tf.math.reduce_sum(t2[0, :3, :3, 0] * filter_[:, :, 0].squeeze())\n",
        "sum_1 = tf.math.reduce_sum(t2[0, :3, :3, 1] * filter_[:, :, 1].squeeze())\n",
        "sum_2 = tf.math.reduce_sum(t2[0, :3, :3, 2] * filter_[:, :, 2].squeeze())\n",
        "\n",
        "sum_fin = sum_0 + sum_1 + sum_2\n",
        "\n",
        "# Check output\n",
        "assert(np.abs(sum_fin - out_multi_numpy[0, 0])<1e-3)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUS0zfvsrED9"
      },
      "source": [
        "## 3.3 LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QzoyL8GOUyF"
      },
      "source": [
        "def leNet5(input_shape):\n",
        "  \"\"\"\n",
        "  Define the LeNet-5\n",
        "  Input:\n",
        "    input_shape [tuple]: shape of input image (height, width, channel)\n",
        "  Output:\n",
        "    model (a tensorflow model instance)\n",
        "  \"\"\"\n",
        "  \n",
        "  # Input layer\n",
        "  X_in = keras.layers.Input(shape=input_shape)\n",
        "  \n",
        "  \n",
        "  # Conv2D layer\n",
        "  X = keras.layers.Conv2D(filters=6, kernel_size=(5, 5), strides=(1, 1), \n",
        "                          activation='tanh', \n",
        "                          padding='same', name=\"leNet5_conv1\")(X_in)\n",
        "  \n",
        "  # Pooling layer\n",
        "  X = keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(1, 1), padding='valid')(X)\n",
        "\n",
        "  # Conv2D\n",
        "  X = keras.layers.Conv2D(filters=16, kernel_size=(5, 5), strides=(1, 1), \n",
        "                          activation='tanh', \n",
        "                          padding='valid', name='leNet5_conv2')(X)\n",
        "\n",
        "  # Pooling layer\n",
        "  X = keras.layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')(X)\n",
        "\n",
        "  # Conv2D\n",
        "  X = keras.layers.Conv2D(filters=120, kernel_size=(5, 5), strides=(1, 1), \n",
        "                          activation='tanh', \n",
        "                          padding='valid', name='leNet5_conv3')(X)\n",
        "  \n",
        "  # Flatten CNN output so that it can be connected to dense layer\n",
        "  X = keras.layers.Flatten()(X)\n",
        "\n",
        "  # Dense\n",
        "  X = keras.layers.Dense(84, activation='tanh')(X)\n",
        "\n",
        "  # Output layer\n",
        "  y = keras.layers.Dense(10, activation='softmax')(X)\n",
        "\n",
        "  model = keras.Model(inputs=X_in, outputs=y, name='leNet5')\n",
        "  return model"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvMcDeXN0FJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa453634-2a20-4c02-be5a-445853f1fce1"
      },
      "source": [
        "model_leNet5 = leNet5((28, 28, 1))\n",
        "model_leNet5.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"leNet5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
            "_________________________________________________________________\n",
            "leNet5_conv1 (Conv2D)        (None, 28, 28, 6)         156       \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 27, 27, 6)         0         \n",
            "_________________________________________________________________\n",
            "leNet5_conv2 (Conv2D)        (None, 23, 23, 16)        2416      \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 11, 11, 16)        0         \n",
            "_________________________________________________________________\n",
            "leNet5_conv3 (Conv2D)        (None, 7, 7, 120)         48120     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5880)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 84)                494004    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                850       \n",
            "=================================================================\n",
            "Total params: 545,546\n",
            "Trainable params: 545,546\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENS4ciCZ1SVP"
      },
      "source": [
        "otptim_algorithm = 'adam'\n",
        "model_leNet5.compile(optimizer=otptim_algorithm,\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMyxABpl126H"
      },
      "source": [
        "train_images = train_images.reshape(train_images.shape + (1, ))\n",
        "test_images = test_images.reshape(test_images.shape + (1, ))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sVYNg3n2V0z"
      },
      "source": [
        "##Note:\n",
        "\n",
        "Training CNN can take a significantly long time on CPU. Luckily Colab offers GPU for short amount of usage. To save time, you can switch `Runtime` to GPU when trains LeNet-5 by clicking on the tag **Runtime** on top of this page and choose **change runtime type**. In the newly opened window, set **hardware acceleration** to GPU. After this selection, you will need to **rerun the whole Notebook from the very top** with **skipping** Section 3.1 and Section 3.2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rp8Yysl1bJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93bf92eb-0be5-4f8d-fcc4-125cbf1af1e6"
      },
      "source": [
        "log_dir = \"logs/fit/\" + \"leNet-5\"\n",
        "%rm -rf \"logs/fit/leNet-5\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "early_stopping =  EarlyStopping(\n",
        "                            monitor='val_loss',\n",
        "                            min_delta=0.0,\n",
        "                            patience=5,\n",
        ")\n",
        "epoch = 30\n",
        "bs = 16\n",
        "# Fitting the dataset\n",
        "history = model_leNet5.fit(x=train_images, \n",
        "            y=train_labels, \n",
        "            epochs=epoch,\n",
        "            batch_size = bs, \n",
        "            validation_data=(val_images, val_labels), \n",
        "            callbacks=[tensorboard_cb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "3713/3713 [==============================] - 13s 3ms/step - loss: 0.3699 - accuracy: 0.8883 - val_loss: 0.2321 - val_accuracy: 0.9317\n",
            "Epoch 2/30\n",
            "3713/3713 [==============================] - 12s 3ms/step - loss: 0.2006 - accuracy: 0.9396 - val_loss: 0.1672 - val_accuracy: 0.9483\n",
            "Epoch 3/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.1539 - accuracy: 0.9530 - val_loss: 0.1399 - val_accuracy: 0.9567\n",
            "Epoch 4/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.1272 - accuracy: 0.9612 - val_loss: 0.1027 - val_accuracy: 0.9683\n",
            "Epoch 5/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.1166 - accuracy: 0.9646 - val_loss: 0.1196 - val_accuracy: 0.9650\n",
            "Epoch 6/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.1021 - accuracy: 0.9688 - val_loss: 0.1010 - val_accuracy: 0.9733\n",
            "Epoch 7/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0923 - accuracy: 0.9727 - val_loss: 0.1006 - val_accuracy: 0.9650\n",
            "Epoch 8/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0950 - accuracy: 0.9708 - val_loss: 0.1112 - val_accuracy: 0.9683\n",
            "Epoch 9/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0898 - accuracy: 0.9727 - val_loss: 0.0793 - val_accuracy: 0.9783\n",
            "Epoch 10/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0829 - accuracy: 0.9740 - val_loss: 0.0858 - val_accuracy: 0.9750\n",
            "Epoch 11/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0802 - accuracy: 0.9755 - val_loss: 0.1017 - val_accuracy: 0.9767\n",
            "Epoch 12/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0738 - accuracy: 0.9764 - val_loss: 0.0873 - val_accuracy: 0.9800\n",
            "Epoch 13/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0743 - accuracy: 0.9772 - val_loss: 0.0813 - val_accuracy: 0.9800\n",
            "Epoch 14/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0719 - accuracy: 0.9779 - val_loss: 0.0904 - val_accuracy: 0.9750\n",
            "Epoch 15/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0763 - accuracy: 0.9772 - val_loss: 0.0885 - val_accuracy: 0.9750\n",
            "Epoch 16/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0736 - accuracy: 0.9770 - val_loss: 0.0886 - val_accuracy: 0.9783\n",
            "Epoch 17/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0691 - accuracy: 0.9782 - val_loss: 0.0883 - val_accuracy: 0.9817\n",
            "Epoch 18/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0699 - accuracy: 0.9788 - val_loss: 0.0784 - val_accuracy: 0.9750\n",
            "Epoch 19/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0685 - accuracy: 0.9784 - val_loss: 0.0953 - val_accuracy: 0.9717\n",
            "Epoch 20/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0706 - accuracy: 0.9784 - val_loss: 0.0803 - val_accuracy: 0.9783\n",
            "Epoch 21/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0644 - accuracy: 0.9799 - val_loss: 0.0496 - val_accuracy: 0.9883\n",
            "Epoch 22/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0652 - accuracy: 0.9798 - val_loss: 0.0722 - val_accuracy: 0.9817\n",
            "Epoch 23/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0641 - accuracy: 0.9805 - val_loss: 0.0636 - val_accuracy: 0.9817\n",
            "Epoch 24/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0601 - accuracy: 0.9816 - val_loss: 0.0790 - val_accuracy: 0.9767\n",
            "Epoch 25/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0608 - accuracy: 0.9805 - val_loss: 0.0647 - val_accuracy: 0.9850\n",
            "Epoch 26/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0590 - accuracy: 0.9815 - val_loss: 0.0722 - val_accuracy: 0.9850\n",
            "Epoch 27/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0607 - accuracy: 0.9812 - val_loss: 0.0791 - val_accuracy: 0.9783\n",
            "Epoch 28/30\n",
            "3713/3713 [==============================] - 11s 3ms/step - loss: 0.0582 - accuracy: 0.9822 - val_loss: 0.0882 - val_accuracy: 0.9767\n",
            "Epoch 29/30\n",
            "1745/3713 [=============>................] - ETA: 5s - loss: 0.0540 - accuracy: 0.9836"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rOhPHEx4Jgd"
      },
      "source": [
        "#4. Compare performance of FCN and LeNet-5\n",
        "# TODO:\n",
        "\n",
        "Now, it's your turn to compare the accuracy of LeNet-5 and FCN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFHNA1Ij4I4k"
      },
      "source": [
        "#check  the training process of Lenet5\n",
        "%tensorboard --logdir logs/fit/leNet-5\n",
        "\n",
        "score_LeNet = model_leNet5.evaluate(test_images, test_labels, verbose=0)\n",
        "print('*** test result for Fully Conv Networks (LeNet5)***')\n",
        "print('Test loss:', score_LeNet[0])\n",
        "print('Test accuracy:', score_LeNet[1])\n",
        "\n",
        "\n",
        "# Get model's prediction for test set\n",
        "predictions_LeNet = model_leNet5.predict(test_images)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdX8vDdATat4"
      },
      "source": [
        "### Reference:\n",
        "https://www.tensorflow.org/tutorials/keras/classification\n",
        "\n",
        "https://www.tensorflow.org/tensorboard/get_started\n",
        "\n",
        "https://engmrk.com/lenet-5-a-classic-cnn-architecture/\n",
        "\n",
        "https://towardsdatascience.com/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac"
      ]
    }
  ]
}